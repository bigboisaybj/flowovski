{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58867dba526843ce9b3aae69a4f1d1e3 b806ef95d125872d1e72133d96464938 1311f004aa1a268e4309afbb26dcaf1b 067980ba0b78d4f6a134fb324d2e5b23 adac9bba354ed200c0549665c3801d4e 6f58344db84efc46ba42efac4a5de557__CONTENT__(1.0000000000000002, 'b806ef95d125872d1e72133d96464938', 'b806ef95d125872d1e72133d96464938') (1.0000000000000002, 'adac9bba354ed200c0549665c3801d4e', 'adac9bba354ed200c0549665c3801d4e') (1.0000000000000002, '6f58344db84efc46ba42efac4a5de557', '6f58344db84efc46ba42efac4a5de557') (1.0000000000000002, '1311f004aa1a268e4309afbb26dcaf1b', '1311f004aa1a268e4309afbb26dcaf1b') (1.0, '58867dba526843ce9b3aae69a4f1d1e3', '58867dba526843ce9b3aae69a4f1d1e3') (1.0, '067980ba0b78d4f6a134fb324d2e5b23', '067980ba0b78d4f6a134fb324d2e5b23') (0.25326515503835467, 'b806ef95d125872d1e72133d96464938', '067980ba0b78d4f6a134fb324d2e5b23') (0.25326515503835467, '067980ba0b78d4f6a134fb324d2e5b23', 'b806ef95d125872d1e72133d96464938') (0.054486497208124214, '58867dba526843ce9b3aae69a4f1d1e3', '1311f004aa1a268e4309afbb26dcaf1b') (0.054486497208124214, '1311f004aa1a268e4309afbb26dcaf1b', '58867dba526843ce9b3aae69a4f1d1e3') (0.048461776072692885, '6f58344db84efc46ba42efac4a5de557', '58867dba526843ce9b3aae69a4f1d1e3') (0.048461776072692885, '58867dba526843ce9b3aae69a4f1d1e3', '6f58344db84efc46ba42efac4a5de557') (0.047789056514460991, '1311f004aa1a268e4309afbb26dcaf1b', '067980ba0b78d4f6a134fb324d2e5b23') (0.047789056514460991, '067980ba0b78d4f6a134fb324d2e5b23', '1311f004aa1a268e4309afbb26dcaf1b') (0.044412708414269926, 'adac9bba354ed200c0549665c3801d4e', '58867dba526843ce9b3aae69a4f1d1e3') (0.044412708414269926, '58867dba526843ce9b3aae69a4f1d1e3', 'adac9bba354ed200c0549665c3801d4e') (0.04284370630765779, 'adac9bba354ed200c0549665c3801d4e', '6f58344db84efc46ba42efac4a5de557') (0.04284370630765779, '6f58344db84efc46ba42efac4a5de557', 'adac9bba354ed200c0549665c3801d4e') (0.039106518729808804, 'b806ef95d125872d1e72133d96464938', '1311f004aa1a268e4309afbb26dcaf1b') (0.039106518729808804, '1311f004aa1a268e4309afbb26dcaf1b', 'b806ef95d125872d1e72133d96464938') (0.026299075950137172, 'b806ef95d125872d1e72133d96464938', '6f58344db84efc46ba42efac4a5de557') (0.026299075950137172, '6f58344db84efc46ba42efac4a5de557', 'b806ef95d125872d1e72133d96464938') (0.024101741339115509, 'b806ef95d125872d1e72133d96464938', 'adac9bba354ed200c0549665c3801d4e') (0.024101741339115509, 'adac9bba354ed200c0549665c3801d4e', 'b806ef95d125872d1e72133d96464938') (0.023866552848016941, '6f58344db84efc46ba42efac4a5de557', '067980ba0b78d4f6a134fb324d2e5b23') (0.023866552848016941, '067980ba0b78d4f6a134fb324d2e5b23', '6f58344db84efc46ba42efac4a5de557') (0.021872459872349037, 'adac9bba354ed200c0549665c3801d4e', '067980ba0b78d4f6a134fb324d2e5b23') (0.021872459872349037, '067980ba0b78d4f6a134fb324d2e5b23', 'adac9bba354ed200c0549665c3801d4e') (0.020245598580488865, 'b806ef95d125872d1e72133d96464938', '58867dba526843ce9b3aae69a4f1d1e3') (0.020245598580488865, '58867dba526843ce9b3aae69a4f1d1e3', 'b806ef95d125872d1e72133d96464938') (0.018372989582489646, '58867dba526843ce9b3aae69a4f1d1e3', '067980ba0b78d4f6a134fb324d2e5b23') (0.018372989582489646, '067980ba0b78d4f6a134fb324d2e5b23', '58867dba526843ce9b3aae69a4f1d1e3') (0.017379119827907148, '6f58344db84efc46ba42efac4a5de557', '1311f004aa1a268e4309afbb26dcaf1b') (0.017379119827907148, '1311f004aa1a268e4309afbb26dcaf1b', '6f58344db84efc46ba42efac4a5de557') (0.01592706342944825, 'adac9bba354ed200c0549665c3801d4e', '1311f004aa1a268e4309afbb26dcaf1b') (0.01592706342944825, '1311f004aa1a268e4309afbb26dcaf1b', 'adac9bba354ed200c0549665c3801d4e')\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "with open('/Users/bryanjordan/Sites/server_upload/total_text/total_text_result.txt','r') as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "word = '__BREAK__' \n",
    "\n",
    "breakLines = []\n",
    "hashes = []\n",
    "\n",
    "for i,line in enumerate(lines):\n",
    "    if word in line: \n",
    "        breakLines.append(i+1)\n",
    "\n",
    "for hashId in breakLines:\n",
    "    hashes.append(lines[hashId])\n",
    "\n",
    "file = open('/Users/bryanjordan/Sites/server_upload/total_text/total_text_result.txt','r') \n",
    " \n",
    "holder = file.read()\n",
    "\n",
    "text = {}\n",
    "listOfHits = []\n",
    "\n",
    "for match in re.finditer(\"__BREAK__\", holder):\n",
    "    result = match.start(), match.end()\n",
    "    listOfHits.append(result)\n",
    "\n",
    "for entry in listOfHits:\n",
    "    if listOfHits.index(entry) == 0:\n",
    "        secondHit = listOfHits[1][0]\n",
    "        temp = holder[entry[1]:secondHit]\n",
    "        title = 'text_' + str(0)\n",
    "        temp = temp.replace(\"\\r\",\" \")\n",
    "        temp = temp.replace(\"\\n\",\" \")\n",
    "        text[title] = temp\n",
    "    else:\n",
    "        \n",
    "        entryIndex = listOfHits.index(entry)\n",
    "        originalIndex = entryIndex\n",
    "        originalIndex += 1\n",
    "        if originalIndex < len(listOfHits):\n",
    "            first_index = listOfHits[entryIndex]\n",
    "            nextIndex = listOfHits[originalIndex]\n",
    "\n",
    "            temp = holder[first_index[1]:nextIndex[0]]\n",
    "            title = 'text_' + str(entryIndex)\n",
    "            temp = temp.replace(\"\\r\",\" \")\n",
    "            temp = temp.replace(\"\\n\",\" \")\n",
    "            text[title] = temp\n",
    "\n",
    "        if originalIndex == len(listOfHits):\n",
    "            first_index = listOfHits[len(listOfHits)-1]\n",
    "\n",
    "            result = holder[first_index[1]:]\n",
    "            title = 'text_' + str(len(listOfHits)-1)\n",
    "            result = result.replace(\"\\r\",\" \")\n",
    "            result = result.replace(\"\\n\",\" \")\n",
    "            text[title] = result\n",
    "            \n",
    "        if originalIndex == len(listOfHits)-1:\n",
    "            originalIndex = len(listOfHits)-1\n",
    "            \n",
    "            first_index = listOfHits[entryIndex]\n",
    "            nextIndex = listOfHits[originalIndex]\n",
    "\n",
    "            temp = holder[first_index[1]:nextIndex[0]]\n",
    "            title = 'text_' + str(entryIndex)\n",
    "            temp = temp.replace(\"\\r\",\" \")\n",
    "            temp = temp.replace(\"\\n\",\" \")\n",
    "            text[title] = temp\n",
    "            \n",
    "total = []\n",
    "\n",
    "for entry in text:\n",
    "    total.append(text[entry])\n",
    "    \n",
    "\n",
    "keyNames = []\n",
    "\n",
    "for key,value in text.items():\n",
    "    keyNames.append(key)\n",
    "\n",
    "file.close() \n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "all_documents = total\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "                \n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    \n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    "\n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "            \n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "skl_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(sklearn_representation.toarray()):\n",
    "    for count_1, doc_1 in enumerate(sklearn_representation.toarray()):\n",
    "        first = hashes[count_0]\n",
    "        second = hashes[count_1]\n",
    "        skl_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), first, second))\n",
    "\n",
    "    \n",
    "final = sorted(skl_tfidf_comparisons, reverse = True)\n",
    "\n",
    "def saveToServer(file, titles):\n",
    "    \n",
    "    fileStr = ' '.join([str(i) for i in file])\n",
    "    titlesStr = ' '.join([str(i) for i in titles])\n",
    "    \n",
    "    save_path = '/Users/bryanjordan/Sites/server_upload/total_tf_idf/'\n",
    "\n",
    "    name_of_file = \"total_tf_idf_results\"\n",
    "\n",
    "    completeName = os.path.join(save_path, name_of_file+\".txt\")         \n",
    "\n",
    "    file1 = open(completeName, \"w+\")\n",
    "    \n",
    "    toWrite = titlesStr + \"__CONTENT__\" + fileStr\n",
    "\n",
    "    file1.write(toWrite)\n",
    "\n",
    "    file1.close()\n",
    "    print(toWrite)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "\n",
    "saveToServer(final, hashes)\n",
    "\n",
    "    \n",
    "#1.if first-key is not in header, add, then go-to 3.\n",
    "#2. elif first-key is in header go-to 3\n",
    "#3 if second value not in column, add then go-to 4.\n",
    "#4 elif second value is in column, add to column\n",
    "\n",
    "\n",
    "#ONCE THIS IS EXECUTED -> UPLOAD to file -> keep column, parse line by line. reading between values.\n",
    "\n",
    "#Thursday: (1) upload, (2) compare, (3) statistical analysis?, (4) track user, (5) FAQ search_results spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
